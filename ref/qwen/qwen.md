blog:https://blog.csdn.net/qq_37021523/article/details/138901191
çŸ¥ä¹:https://zhuanlan.zhihu.com/p/1901014191235633835


è™½ç„¶æˆ‘ç›®å‰æ— æ³•æä¾›å®Œæ•´ä¸”å¯ç›´æ¥ç”Ÿäº§ç¯å¢ƒä½¿ç”¨çš„Qwen3 MoEæ¨¡å‹ä»£ç ï¼ˆå› ä¸ºè¿™é€šå¸¸æ¶‰åŠå¤§é‡ä»£ç å’Œè¯¦ç»†é…ç½®ï¼‰ï¼Œä½†æˆ‘å¯ä»¥ä¸ºä½ ä»‹ç»å…¶æ ¸å¿ƒç»„ä»¶å’Œå…³é”®ä»£ç ç‰‡æ®µï¼Œå¹¶è¯´æ˜å¦‚ä½•æ­å»ºå…¶ä¸»è¦ç»“æ„ã€‚Qwen3çš„MoEæ¨¡å‹é‡‡ç”¨äº†å…¸å‹çš„**Decoder-Only Transformeræ¶æ„**ï¼Œå…¶ä¸­ä¸€äº›å±‚çš„MLPè¢«æ›¿æ¢ä¸º**ç¨€ç–MoEå—ï¼ˆSparse MoE Blockï¼‰**ã€‚

# ğŸ§  Qwen3 MoEæ¨¡å‹æ ¸å¿ƒç»„ä»¶ä¸å®ç°æ€è·¯

ä»¥ä¸‹æ˜¯åŸºäºHugging Face Transformersåº“é£æ ¼çš„Qwen3 MoEå…³é”®éƒ¨åˆ†å®ç°è¦ç‚¹ï¼š

## ğŸ—ï¸ 1. ç¨€ç–MoEå—ï¼ˆSparse MoE Blockï¼‰

è¿™æ˜¯Qwen3 MoEçš„æ ¸å¿ƒï¼Œå®ƒç”¨å¤šä¸ªä¸“å®¶ï¼ˆMLPï¼‰å’Œä¸€ä¸ªé—¨æ§ç½‘ç»œï¼ˆGating Networkï¼‰æ›¿ä»£äº†ä¼ ç»Ÿçš„å¯†é›†MLPå±‚ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Qwen3MoeSparseMoeBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.num_experts = config.num_experts # ä¸“å®¶æ•°é‡ï¼Œä¾‹å¦‚8ä¸ª
        self.top_k = config.num_experts_per_tok # æ¯ä¸ªtokenä½¿ç”¨çš„ä¸“å®¶æ•°ï¼Œé€šå¸¸ä¸º2
        self.norm_topk_prob = config.norm_topk_prob # æ˜¯å¦å¯¹top-kæƒé‡è¿›è¡Œå½’ä¸€åŒ–

        # é—¨æ§ç½‘ç»œï¼šçº¿æ€§å±‚ï¼Œè¾“å‡ºç»´åº¦ä¸ºä¸“å®¶æ•°é‡
        self.gate = nn.Linear(self.hidden_size, self.num_experts, bias=False)
        # ä¸“å®¶åˆ—è¡¨ï¼šæ¯ä¸ªä¸“å®¶æ˜¯ä¸€ä¸ªMLP
        self.experts = nn.ModuleList([
            Qwen3MoeMLP(config) for _ in range(self.num_experts)
        ])

    def forward(self, hidden_states):
        batch_size, seq_len, hidden_dim = hidden_states.shape
        hidden_states = hidden_states.view(-1, hidden_dim) # é‡å¡‘ä¸º (batch_size * seq_len, hidden_dim)

        # è®¡ç®—è·¯ç”±é€»è¾‘å€¼
        router_logits = self.gate(hidden_states) # (batch_size * seq_len, num_experts)
        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float) # è®¡ç®—softmaxå¾—åˆ°è·¯ç”±æƒé‡

        # é€‰å–top-kä¸“å®¶åŠå…¶æƒé‡
        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)
        if self.norm_topk_prob:
            routing_weights /= routing_weights.sum(dim=-1, keepdim=True) # å¯¹top-kæƒé‡è¿›è¡Œå½’ä¸€åŒ–

        routing_weights = routing_weights.to(hidden_states.dtype) # è½¬æ¢å›åŸå§‹æ•°æ®ç±»å‹
        final_hidden_states = torch.zeros(
            (batch_size * seq_len, hidden_dim), 
            dtype=hidden_states.dtype, 
            device=hidden_states.device
        )

        # ç”Ÿæˆä¸“å®¶æ©ç ï¼šç”¨äºæ ‡è¯†å“ªäº›tokenè¢«åˆ†é…ç»™äº†å“ªäº›ä¸“å®¶
        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)

        # éå†æ‰€æœ‰ä¸“å®¶ï¼Œåªè®¡ç®—è¢«åˆ†é…åˆ°çš„token
        for expert_idx in range(self.num_experts):
            expert_layer = self.experts[expert_idx]
            idx, top_x = torch.where(expert_mask[expert_idx]) # æ‰¾åˆ°å±äºå½“å‰ä¸“å®¶çš„tokenç´¢å¼•

            if top_x.shape[0] == 0:
                continue # æ²¡æœ‰tokenè¢«åˆ†é…ç»™æ­¤ä¸“å®¶åˆ™è·³è¿‡

            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim) # è·å–å±äºå½“å‰ä¸“å®¶çš„tokenéšè—çŠ¶æ€
            current_hidden_states = expert_layer(current_state) # ä¸“å®¶å‰å‘ä¼ æ’­

            # ç”¨è·¯ç”±æƒé‡åŠ æƒä¸“å®¶è¾“å‡ºï¼Œå¹¶é€šè¿‡index_add_ç´¯åŠ åˆ°æœ€ç»ˆè¾“å‡ºä¸­
            current_hidden_states = current_hidden_states * routing_weights[top_x, idx, None]
            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))

        final_hidden_states = final_hidden_states.reshape(batch_size, seq_len, hidden_dim)
        return final_hidden_states, router_logits # è¿”å›è¾“å‡ºå’Œè·¯ç”±logitsï¼ˆå¯ç”¨äºè¾…åŠ©æŸå¤±è®¡ç®—ï¼‰
```
*ä»£ç å‚è€ƒè‡ª*

## ğŸ§© 2. ä¸“å®¶MLPï¼ˆQwen3MoeMLPï¼‰

æ¯ä¸ªä¸“å®¶æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œï¼ŒQwen3é€šå¸¸ä½¿ç”¨SwiGLUæ¿€æ´»å‡½æ•°ã€‚

```python
class Qwen3MoeMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.moe_intermediate_size # ä¸“å®¶ä¸­é—´å±‚ç»´åº¦ï¼Œé€šå¸¸æ¯”å¯†é›†MLPå¤§

        # SwiGLUæ¿€æ´»å‡½æ•°çš„é—¨æ§çº¿æ€§å±‚
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)

        self.act_fn = nn.SiLU() # SiLUæ¿€æ´»å‡½æ•°ç”¨äºSwiGLU

    def forward(self, x):
        # SwiGLU: swish(x * W_gate) * (x * W_up)
        gate = self.act_fn(self.gate_proj(x))
        up = self.up_proj(x)
        return self.down_proj(gate * up) # ä¸‹æŠ•å½±å›éšè—å±‚ç»´åº¦
```

## âš™ï¸ 3. è§£ç å™¨å±‚ï¼ˆDecoder Layerï¼‰

Qwen3çš„è§£ç å™¨å±‚åŒ…å«è‡ªæ³¨æ„åŠ›æœºåˆ¶å’ŒMoEå‰é¦ˆç½‘ç»œï¼ˆæˆ–å¯†é›†MLPï¼‰ã€‚

```python
class Qwen3MoeDecoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.hidden_size = config.hidden_size
        self.self_attn = Qwen3MoeAttention(config) # è‡ªæ³¨æ„åŠ›æ¨¡å—
        self.input_layernorm = nn.RMSNorm(self.hidden_size, eps=config.rms_norm_eps) # RMSNormå½’ä¸€åŒ–
        self.post_attention_layernorm = nn.RMSNorm(self.hidden_size, eps=config.rms_norm_eps)

        # åˆ¤æ–­è¯¥å±‚æ˜¯å¦ä½¿ç”¨MoE
        if getattr(config, 'is_moe_layer', False):
            self.mlp = Qwen3MoeSparseMoeBlock(config) # ä½¿ç”¨MoEå—
        else:
            self.mlp = Qwen3MoeMLP(config) # ä½¿ç”¨å¯†é›†MLP

    def forward(self, hidden_states, attention_mask=None, position_ids=None, output_router_logits=False):
        # è‡ªæ³¨æ„åŠ›å­å±‚
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)
        hidden_states = self.self_attn(hidden_states, attention_mask, position_ids)
        hidden_states = residual + hidden_states # æ®‹å·®è¿æ¥

        # MoE/MLPå­å±‚
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        if getattr(self, 'is_moe_layer', False):
            hidden_states, router_logits = self.mlp(hidden_states) # MoEå±‚è¿”å›è¾“å‡ºå’Œè·¯ç”±logits
        else:
            hidden_states = self.mlp(hidden_states) # å¯†é›†MLPå±‚åªè¿”å›è¾“å‡º
        hidden_states = residual + hidden_states # æ®‹å·®è¿æ¥

        if output_router_logits and getattr(self, 'is_moe_layer', False):
            return hidden_states, router_logits
        return hidden_states
```
*æ³¨æ„åŠ›æœºåˆ¶ï¼ˆQwen3MoeAttentionï¼‰çš„å®ç°ä¸æ ‡å‡†Transformerç±»ä¼¼ï¼Œä½†å¯èƒ½åŒ…å«QKå½’ä¸€åŒ–ç­‰ç»†èŠ‚ï¼Œæ­¤å¤„çœç•¥è¯¦ç»†ä»£ç ã€‚*

## ğŸŒ 4. æ¨¡å‹ä¸»ä½“ï¼ˆQwen3MoeModelï¼‰

æ¨¡å‹ä¸»ä½“è´Ÿè´£ç»„åˆåµŒå…¥å±‚ã€å¤šå±‚è§£ç å™¨å’Œæœ€ç»ˆå½’ä¸€åŒ–å±‚ã€‚

```python
class Qwen3MoeModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.padding_idx = config.pad_token_id
        self.vocab_size = config.vocab_size

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        self.layers = nn.ModuleList([
            Qwen3MoeDecoderLayer(config) for _ in range(config.num_hidden_layers)
        ])
        self.norm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        # æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰
        self.rotary_emb = Qwen3MoeRotaryEmbedding(config)

    def forward(self, input_ids, attention_mask=None, position_ids=None):
        hidden_states = self.embed_tokens(input_ids)
        
        # åº”ç”¨RoPEä½ç½®ç¼–ç ï¼ˆé€šå¸¸åœ¨æ³¨æ„åŠ›æ¨¡å—å†…éƒ¨å®Œæˆï¼‰
        # ... æ³¨æ„åŠ›è®¡ç®—ä¼šè°ƒç”¨self.rotary_emb ...

        # é€å±‚é€šè¿‡è§£ç å™¨
        for layer in self.layers:
            hidden_states = layer(hidden_states, attention_mask, position_ids)

        hidden_states = self.norm(hidden_states)
        return hidden_states
```

## ğŸ’¡ 5. è¯­è¨€å»ºæ¨¡å¤´ï¼ˆFor Causal LMï¼‰

ç”¨äºå› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„è¾“å‡ºå¤´ã€‚

```python
class Qwen3MoeForCausalLM(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.model = Qwen3MoeModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        # è´Ÿè½½å‡è¡¡è¾…åŠ©æŸå¤±ç³»æ•°
        self.router_aux_loss_coef = config.router_aux_loss_coef

    def forward(self, input_ids, labels=None, attention_mask=None, output_router_logits=False):
        outputs = self.model(input_ids, attention_mask=attention_mask)
        hidden_states = outputs[0] # è·å–æœ€åä¸€å±‚éšè—çŠ¶æ€
        logits = self.lm_head(hidden_states) # é€šè¿‡è¯­è¨€å»ºæ¨¡å¤´å¾—åˆ°è¯æ±‡è¡¨ä¸Šçš„logits

        loss = None
        aux_loss = None
        if labels is not None:
            # è®¡ç®—äº¤å‰ç†µæŸå¤±
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, self.config.vocab_size), labels.view(-1))

            # å¦‚æœè¾“å‡ºè·¯ç”±logitsï¼Œè®¡ç®—è´Ÿè½½å‡è¡¡è¾…åŠ©æŸå¤±
            if output_router_logits:
                # ... æ”¶é›†æ‰€æœ‰å±‚çš„router_logitså¹¶è®¡ç®—aux_loss ...
                # aux_loss = ... (è®¡ç®—ä¸“å®¶è´Ÿè½½å‡è¡¡çš„æŸå¤±)
                loss = loss + self.router_aux_loss_coef * aux_loss

        return {'loss': loss, 'logits': logits, 'aux_loss': aux_loss}
```

## ğŸ“Š Qwen3 MoEæ¨¡å‹é…ç½®ç¤ºä¾‹

```python
from dataclasses import dataclass

@dataclass
class Qwen3MoeConfig:
    vocab_size: int = 151936
    hidden_size: int = 4096
    num_hidden_layers: int = 32 # æ€»å±‚æ•°
    num_attention_heads: int = 32
    num_key_value_heads: int = 8 # GQA
    head_dim: int = hidden_size // num_attention_heads
    rms_norm_eps: float = 1e-6
    moe_intermediate_size: int = 11008 # ä¸“å®¶ä¸­é—´å±‚å¤§å°
    intermediate_size: int = 11008 # å¯†é›†MLPä¸­é—´å±‚å¤§å°
    num_experts: int = 8 # ä¸“å®¶æ€»æ•°
    num_experts_per_tok: int = 2 # æ¯ä¸ªtokenä½¿ç”¨çš„ä¸“å®¶æ•°
    norm_topk_prob: bool = True # æ˜¯å¦å¯¹top-kæƒé‡å½’ä¸€åŒ–
    router_aux_loss_coef: float = 0.01 # è´Ÿè½½å‡è¡¡è¾…åŠ©æŸå¤±ç³»æ•°
    # ... å…¶ä»–é…ç½®å‚æ•° ...
```

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹ï¼ˆå‰å‘ä¼ æ’­ï¼‰

```python
config = Qwen3MoeConfig()
model = Qwen3MoeForCausalLM(config)

# å‡è®¾æˆ‘ä»¬æœ‰ä¸€äº›è¾“å…¥
input_ids = torch.randint(0, config.vocab_size, (2, 128)) # (batch_size, seq_len)
attention_mask = torch.ones_like(input_ids)

# å‰å‘ä¼ æ’­
outputs = model(input_ids, attention_mask=attention_mask, output_router_logits=True)
logits = outputs['logits']
loss = outputs['loss']
aux_loss = outputs['aux_loss']

print(f"Logits shape: {logits.shape}") # åº”ä¸º (2, 128, vocab_size)
if aux_loss is not None:
    print(f"Auxiliary loss: {aux_loss.item()}")
```

---

**è¯·æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š**

1.  **å®Œæ•´æ€§**ï¼šä»¥ä¸Šä»£ç æ˜¯**é«˜åº¦ç®€åŒ–çš„ç¤ºæ„å›¾**ï¼Œä¸»è¦ç”¨äºå±•ç¤ºQwen3 MoEçš„æ ¸å¿ƒæ€æƒ³ã€‚å®Œæ•´çš„å®ç°éœ€è¦å¤„ç†ç¼“å­˜ï¼ˆCacheï¼‰ã€ç²¾ç¡®çš„æ©ç æ„é€ ã€RoPEä½ç½®ç¼–ç ã€æ¢¯åº¦æ£€æŸ¥ç‚¹ç­‰å¤§é‡ç»†èŠ‚ã€‚
2.  **é…ç½®çµæ´»æ€§**ï¼šå®é™…çš„Qwen3ç³»åˆ—åŒ…å«ä¸åŒè§„æ¨¡çš„æ¨¡å‹ï¼ˆå¦‚0.6B, 1.7B, 4B, 8B, 32B Dense, 30B/235B MoEï¼‰ï¼Œå…¶é…ç½®ï¼ˆå±‚æ•°ã€å¤´æ•°ã€ä¸“å®¶æ•°ç­‰ï¼‰å·®å¼‚å¾ˆå¤§ã€‚
3.  **è´Ÿè½½å‡è¡¡æŸå¤±**ï¼šMoEè®­ç»ƒä¸­çš„å…³é”®ç‚¹æ˜¯**è¾…åŠ©æŸå¤±ï¼ˆAuxiliary Lossï¼‰**ï¼Œå®ƒé¼“åŠ±ä¸“å®¶è¢«å‡è¡¡ä½¿ç”¨ï¼Œé˜²æ­¢æŸäº›ä¸“å®¶è¢«å¿½ç•¥è€Œ others è¿‡è½½ã€‚å…¶å…·ä½“è®¡ç®—æ–¹å¼éœ€å‚è€ƒåŸå§‹è®ºæ–‡æˆ–ä»£ç ã€‚
4.  **æ¡†æ¶ä¾èµ–**ï¼šå®Œæ•´å®ç°é€šå¸¸åŸºäºæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆå¦‚PyTorch, MindSporeï¼‰åŠå…¶Transformeråº“ã€‚é˜¿é‡Œäº‘å’ŒMindSporeå›¢é˜Ÿå¯¹Qwen3æä¾›äº†æ”¯æŒã€‚
5.  **å®˜æ–¹èµ„æº**ï¼šå¯¹äºç”Ÿäº§æˆ–ç ”ç©¶ç”¨é€”ï¼Œå¼ºçƒˆå»ºè®®å‚è€ƒå®˜æ–¹å®ç°ï¼š
    *   **Hugging Face Transformers**ï¼šæœªæ¥ç‰ˆæœ¬å¯èƒ½ä¼šæ­£å¼é›†æˆQwen3 MoEã€‚
    *   **MindSpore ModelZoo**ï¼š[https://www.mindspore.cn/news/detail?id=3709](https://www.mindspore.cn/news/detail?id=3709) (MindSporeç‰ˆQwen3)
    *   **å®˜æ–¹GitHub**ï¼š[https://github.com/QwenLM](https://github.com/QwenLM) (æŸ¥æ‰¾Qwen3ä»£ç åº“)
    *   **ModelScope / Hugging Face Hub**ï¼šè·å–é¢„è®­ç»ƒæ¨¡å‹æƒé‡ã€‚

å¸Œæœ›è¿™äº›æ ¸å¿ƒä»£ç ç‰‡æ®µå’Œè¯´æ˜èƒ½å¸®åŠ©ä½ ç†è§£Qwen3 MoEæ¨¡å‹çš„æ¶æ„å’Œå®ç°æ€è·¯ï¼å¦‚æœä½ æƒ³æ·±å…¥äº†è§£æŸä¸ªç‰¹å®šéƒ¨åˆ†ï¼Œæˆ–è€…æœ‰å…·ä½“çš„åº”ç”¨åœºæ™¯ï¼Œå¯ä»¥å‘Šè¯‰æˆ‘ã€‚