{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码构建部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Self_Attention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout):\n",
    "        super(Self_Attention, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.output_LayerNorm = nn.LayerNorm(d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.Dropout1 = nn.Dropout(dropout)\n",
    "        self.Dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        residual = q  # 保存残差\n",
    "        batch_size, seq_len, d_model = q.size()\n",
    "        \n",
    "        # 多头注意力计算（与你的逻辑一致）\n",
    "        q = self.W_q(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.W_k(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.W_v(v).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        attn = torch.matmul(q, k.transpose(2, 3)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1)\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        scores = torch.softmax(attn, dim=-1)\n",
    "        scores = self.Dropout1(scores)\n",
    "        \n",
    "        output = torch.matmul(scores, v).transpose(2,1).contiguous().view(batch_size, seq_len, d_model)\n",
    "        output = self.W_o(output)\n",
    "        output = self.Dropout2(output)\n",
    "        \n",
    "        # 补全残差连接（BERT的核心步骤）\n",
    "        output = output + residual  # 关键：输出 + 输入残差\n",
    "        output = self.output_LayerNorm(output)  # 再做LayerNorm\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_heads, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = Self_Attention(d_model=d_model, n_heads=n_heads, dropout=dropout)\n",
    "        # 前馈网络添加激活函数（BERT用GELU）\n",
    "        self.intermediate = nn.Linear(d_model, d_ff)\n",
    "        self.intermediate_act_fn = nn.GELU()  # 新增：激活函数\n",
    "        self.output = nn.Linear(d_ff, d_model)\n",
    "        self.output_LayerNorm = nn.LayerNorm(d_model)  # 更清晰的命名\n",
    "        self.dropout = nn.Dropout(dropout)  # BERT在前馈网络后也会加dropout\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 1. 自注意力层（假设Self_Attention已包含残差+Norm）\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        \n",
    "        # 2. 前馈网络\n",
    "        ff_output = self.intermediate(attn_output)\n",
    "        ff_output = self.intermediate_act_fn(ff_output)  # 应用激活函数\n",
    "        ff_output = self.dropout(ff_output)  # 新增：dropout增强泛化性\n",
    "        ff_output = self.output(ff_output)\n",
    "        \n",
    "        # 3. 前馈网络的残差+Norm\n",
    "        ff_output = ff_output + attn_output  # 残差连接（基准是自注意力的输出）\n",
    "        ff_output = self.output_LayerNorm(ff_output)  # LayerNorm\n",
    "        \n",
    "        return ff_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, vocab_size, dropout, pad_token_id=0):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
    "        self.position_embeddings = nn.Embedding(max_len, d_model)\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=pad_token_id) \n",
    "        self.token_type_embeddings = nn.Embedding(2, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"position_ids\", torch.arange(max_len).expand((1, -1)), persistent=False\n",
    "        )\n",
    "    def forward(self, x, token_type_ids):\n",
    "        seq_length = x.size(1)\n",
    "        position_ids = self.position_ids[:, 0 : seq_length + 0]\n",
    "        inputs_embeds = self.word_embeddings(x)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        embeddings = inputs_embeds + token_type_embeddings\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    \"\"\"对应bert.pooler.dense参数\"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)  # weight: [768,768], bias: [768]\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: [batch_size, seq_len, d_model]\n",
    "        # 取[CLS] token的输出（第一个位置）\n",
    "        cls_token = hidden_states[:, 0, :]  # [batch_size, d_model]\n",
    "        pooled_output = self.dense(cls_token)  # 线性变换\n",
    "        pooled_output = self.activation(pooled_output)  # tanh激活\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    \"\"\"对应cls.predictions.transform参数\"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(d_model, d_model)  # weight: [768,768], bias: [768]\n",
    "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)  # gamma: [768], beta: [768]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = F.gelu(hidden_states)  # BERT用GELU激活\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    \"\"\"对应cls.predictions参数（MLM任务头）\"\"\"\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(d_model)\n",
    "        # 预测头：映射到词表大小（复用词嵌入矩阵的转置，这里先定义为独立参数）\n",
    "        self.decoder = nn.Linear(d_model, vocab_size, bias=False)  # 实际BERT中会复用embedding权重\n",
    "        self.bias = nn.Parameter(torch.zeros(vocab_size))  # cls.predictions.bias: [30522]\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # hidden_states: [batch_size, seq_len, d_model]（编码器输出）\n",
    "        hidden_states = self.transform(hidden_states)  # 特征变换\n",
    "        logits = self.decoder(hidden_states) + self.bias  # 计算词表logits\n",
    "        return logits\n",
    "\n",
    "\n",
    "class BertOnlyNSPHead(nn.Module):\n",
    "    \"\"\"对应cls.seq_relationship参数（NSP任务头）\"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.seq_relationship = nn.Linear(d_model, 2)  # weight: [2,768], bias: [2]\n",
    "\n",
    "    def forward(self, pooled_output):\n",
    "        # pooled_output: [batch_size, d_model]（池化层输出）\n",
    "        logits = self.seq_relationship(pooled_output)  # 二分类logits\n",
    "        return logits\n",
    "class BertPreTrainingHeads(nn.Module):\n",
    "    \"\"\"整合MLM和NSP任务头，对应cls整体\"\"\"\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(d_model, vocab_size)  # MLM头\n",
    "        self.seq_relationship = BertOnlyNSPHead(d_model)  # NSP头\n",
    "\n",
    "    def forward(self, hidden_states, pooled_output):\n",
    "        mlm_logits = self.predictions(hidden_states)  # [batch_size, seq_len, vocab_size]\n",
    "        nsp_logits = self.seq_relationship(pooled_output)  # [batch_size, 2]\n",
    "        return hidden_states, pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, d_model, max_len, vocab_size, d_ff, n_heads, num_hidden_layers, dropout=0.1):\n",
    "        super(Bert, self).__init__()\n",
    "        self.embeddings = Embedding(d_model, max_len, vocab_size, dropout)\n",
    "        self.encoder = nn.ModuleList([EncoderLayer(d_model=d_model, d_ff=d_ff, n_heads=n_heads, dropout=dropout) for _ in range(num_hidden_layers)])\n",
    "        self.pooler = BertPooler(d_model)\n",
    "        self.cls = BertPreTrainingHeads(d_model, vocab_size)\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask=None):\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        \n",
    "        encoder_output = embedding_output\n",
    "        for layer in self.encoder:\n",
    "            encoder_output = layer(encoder_output, attention_mask)\n",
    "        pooled_output = self.pooler(encoder_output)\n",
    "        mlm_logits, nsp_logits = self.cls(encoder_output, pooled_output)\n",
    "        return mlm_logits, nsp_logits\n",
    "\n",
    "    def load_dict(self, safetensor_path):\n",
    "        from safetensors.torch import save_file, load_file\n",
    "        loaded_state = load_file(safetensor_path)\n",
    "        self.embeddings.LayerNorm.weight = torch.nn.Parameter(loaded_state['bert.embeddings.LayerNorm.gamma'])\n",
    "        self.embeddings.LayerNorm.bias = torch.nn.Parameter(loaded_state['bert.embeddings.LayerNorm.beta'])\n",
    "        self.embeddings.position_embeddings.weight = torch.nn.Parameter(loaded_state['bert.embeddings.position_embeddings.weight'])\n",
    "        self.embeddings.word_embeddings.weight = torch.nn.Parameter(loaded_state['bert.embeddings.word_embeddings.weight'])\n",
    "        self.embeddings.token_type_embeddings.weight = torch.nn.Parameter(loaded_state['bert.embeddings.token_type_embeddings.weight'])\n",
    "        for i in range(12):\n",
    "            self.encoder[i].attention.output_LayerNorm.weight = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.attention.output.LayerNorm.gamma'])\n",
    "            self.encoder[i].attention.output_LayerNorm.bias = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.attention.output.LayerNorm.beta'])\n",
    "            self.encoder[i].attention.W_o.weight = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.attention.output.dense.weight'])\n",
    "            self.encoder[i].attention.W_o.bias = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.attention.output.dense.bias'])\n",
    "            self.encoder[i].attention.W_q.weight = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.attention.self.query.weight'])\n",
    "            self.encoder[i].attention.W_q.bias = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.attention.self.query.bias'])\n",
    "            self.encoder[i].attention.W_k.weight = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.attention.self.key.weight'])\n",
    "            self.encoder[i].attention.W_k.bias = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.attention.self.key.bias'])\n",
    "            self.encoder[i].attention.W_v.weight = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.attention.self.value.weight'])\n",
    "            self.encoder[i].attention.W_v.bias = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.attention.self.value.bias'])\n",
    "            self.encoder[i].intermediate.weight = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.intermediate.dense.weight'])\n",
    "            self.encoder[i].intermediate.bias = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.intermediate.dense.bias'])\n",
    "            self.encoder[i].output.weight = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.output.dense.weight'])\n",
    "            self.encoder[i].output.bias = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.output.dense.bias'])\n",
    "            self.encoder[i].output_LayerNorm.weight = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.output.LayerNorm.gamma'])\n",
    "            self.encoder[i].output_LayerNorm.bias = torch.nn.Parameter(loaded_state[f'bert.encoder.layer.{i}.output.LayerNorm.beta'])\n",
    "        self.pooler.dense.weight = torch.nn.Parameter(loaded_state['bert.pooler.dense.weight'])\n",
    "        self.pooler.dense.bias = torch.nn.Parameter(loaded_state['bert.pooler.dense.bias'])\n",
    "        self.cls.predictions.transform.dense.weight = torch.nn.Parameter(loaded_state['cls.predictions.transform.dense.weight'])\n",
    "        self.cls.predictions.transform.dense.bias = torch.nn.Parameter(loaded_state['cls.predictions.transform.dense.bias'])\n",
    "        self.cls.predictions.transform.LayerNorm.weight = torch.nn.Parameter(loaded_state['cls.predictions.transform.LayerNorm.gamma'])\n",
    "        self.cls.predictions.transform.LayerNorm.bias = torch.nn.Parameter(loaded_state['cls.predictions.transform.LayerNorm.beta'])\n",
    "        self.cls.predictions.decoder.weight = torch.nn.Parameter(loaded_state['bert.embeddings.word_embeddings.weight'])\n",
    "        self.cls.predictions.bias = torch.nn.Parameter(loaded_state['cls.predictions.bias'])\n",
    "        self.cls.seq_relationship.seq_relationship.weight = torch.nn.Parameter(loaded_state['cls.seq_relationship.weight'])\n",
    "        self.cls.seq_relationship.seq_relationship.bias = torch.nn.Parameter(loaded_state['cls.seq_relationship.bias'])\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看模型的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bert(\n",
    "    d_model=768,\n",
    "    max_len=512,\n",
    "    vocab_size=30522,\n",
    "    d_ff=3072,\n",
    "    n_heads=12,\n",
    "    num_hidden_layers=12,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.LayerNorm.weight: torch.Size([768])\n",
      "embeddings.LayerNorm.bias: torch.Size([768])\n",
      "embeddings.position_embeddings.weight: torch.Size([512, 768])\n",
      "embeddings.word_embeddings.weight: torch.Size([30522, 768])\n",
      "embeddings.token_type_embeddings.weight: torch.Size([2, 768])\n",
      "encoder.0.attention.output_LayerNorm.weight: torch.Size([768])\n",
      "encoder.0.attention.output_LayerNorm.bias: torch.Size([768])\n",
      "encoder.0.attention.W_o.weight: torch.Size([768, 768])\n",
      "encoder.0.attention.W_o.bias: torch.Size([768])\n",
      "encoder.0.attention.W_q.weight: torch.Size([768, 768])\n",
      "encoder.0.attention.W_q.bias: torch.Size([768])\n",
      "encoder.0.attention.W_k.weight: torch.Size([768, 768])\n",
      "encoder.0.attention.W_k.bias: torch.Size([768])\n",
      "encoder.0.attention.W_v.weight: torch.Size([768, 768])\n",
      "encoder.0.attention.W_v.bias: torch.Size([768])\n",
      "encoder.0.LayerNorm.weight: torch.Size([768])\n",
      "encoder.0.LayerNorm.bias: torch.Size([768])\n",
      "encoder.0.intermediate.weight: torch.Size([3072, 768])\n",
      "encoder.0.intermediate.bias: torch.Size([3072])\n",
      "encoder.0.output.weight: torch.Size([768, 3072])\n",
      "encoder.0.output.bias: torch.Size([768])\n",
      "encoder.1.attention.output_LayerNorm.weight: torch.Size([768])\n",
      "encoder.1.attention.output_LayerNorm.bias: torch.Size([768])\n",
      "encoder.1.attention.W_o.weight: torch.Size([768, 768])\n",
      "encoder.1.attention.W_o.bias: torch.Size([768])\n",
      "encoder.1.attention.W_q.weight: torch.Size([768, 768])\n",
      "encoder.1.attention.W_q.bias: torch.Size([768])\n",
      "encoder.1.attention.W_k.weight: torch.Size([768, 768])\n",
      "encoder.1.attention.W_k.bias: torch.Size([768])\n",
      "encoder.1.attention.W_v.weight: torch.Size([768, 768])\n",
      "encoder.1.attention.W_v.bias: torch.Size([768])\n",
      "encoder.1.LayerNorm.weight: torch.Size([768])\n",
      "encoder.1.LayerNorm.bias: torch.Size([768])\n",
      "encoder.1.intermediate.weight: torch.Size([3072, 768])\n",
      "encoder.1.intermediate.bias: torch.Size([3072])\n",
      "encoder.1.output.weight: torch.Size([768, 3072])\n",
      "encoder.1.output.bias: torch.Size([768])\n",
      "encoder.2.attention.output_LayerNorm.weight: torch.Size([768])\n",
      "encoder.2.attention.output_LayerNorm.bias: torch.Size([768])\n",
      "encoder.2.attention.W_o.weight: torch.Size([768, 768])\n",
      "encoder.2.attention.W_o.bias: torch.Size([768])\n",
      "encoder.2.attention.W_q.weight: torch.Size([768, 768])\n",
      "encoder.2.attention.W_q.bias: torch.Size([768])\n",
      "encoder.2.attention.W_k.weight: torch.Size([768, 768])\n",
      "encoder.2.attention.W_k.bias: torch.Size([768])\n",
      "encoder.2.attention.W_v.weight: torch.Size([768, 768])\n",
      "encoder.2.attention.W_v.bias: torch.Size([768])\n",
      "encoder.2.LayerNorm.weight: torch.Size([768])\n",
      "encoder.2.LayerNorm.bias: torch.Size([768])\n",
      "encoder.2.intermediate.weight: torch.Size([3072, 768])\n",
      "encoder.2.intermediate.bias: torch.Size([3072])\n",
      "encoder.2.output.weight: torch.Size([768, 3072])\n",
      "encoder.2.output.bias: torch.Size([768])\n",
      "encoder.3.attention.output_LayerNorm.weight: torch.Size([768])\n",
      "encoder.3.attention.output_LayerNorm.bias: torch.Size([768])\n",
      "encoder.3.attention.W_o.weight: torch.Size([768, 768])\n",
      "encoder.3.attention.W_o.bias: torch.Size([768])\n",
      "encoder.3.attention.W_q.weight: torch.Size([768, 768])\n",
      "encoder.3.attention.W_q.bias: torch.Size([768])\n",
      "encoder.3.attention.W_k.weight: torch.Size([768, 768])\n",
      "encoder.3.attention.W_k.bias: torch.Size([768])\n",
      "encoder.3.attention.W_v.weight: torch.Size([768, 768])\n",
      "encoder.3.attention.W_v.bias: torch.Size([768])\n",
      "encoder.3.LayerNorm.weight: torch.Size([768])\n",
      "encoder.3.LayerNorm.bias: torch.Size([768])\n",
      "encoder.3.intermediate.weight: torch.Size([3072, 768])\n",
      "encoder.3.intermediate.bias: torch.Size([3072])\n",
      "encoder.3.output.weight: torch.Size([768, 3072])\n",
      "encoder.3.output.bias: torch.Size([768])\n",
      "encoder.4.attention.output_LayerNorm.weight: torch.Size([768])\n",
      "encoder.4.attention.output_LayerNorm.bias: torch.Size([768])\n",
      "encoder.4.attention.W_o.weight: torch.Size([768, 768])\n",
      "encoder.4.attention.W_o.bias: torch.Size([768])\n",
      "encoder.4.attention.W_q.weight: torch.Size([768, 768])\n",
      "encoder.4.attention.W_q.bias: torch.Size([768])\n",
      "encoder.4.attention.W_k.weight: torch.Size([768, 768])\n",
      "encoder.4.attention.W_k.bias: torch.Size([768])\n",
      "encoder.4.attention.W_v.weight: torch.Size([768, 768])\n",
      "encoder.4.attention.W_v.bias: torch.Size([768])\n",
      "encoder.4.LayerNorm.weight: torch.Size([768])\n",
      "encoder.4.LayerNorm.bias: torch.Size([768])\n",
      "encoder.4.intermediate.weight: torch.Size([3072, 768])\n",
      "encoder.4.intermediate.bias: torch.Size([3072])\n",
      "encoder.4.output.weight: torch.Size([768, 3072])\n",
      "encoder.4.output.bias: torch.Size([768])\n",
      "encoder.5.attention.output_LayerNorm.weight: torch.Size([768])\n",
      "encoder.5.attention.output_LayerNorm.bias: torch.Size([768])\n",
      "encoder.5.attention.W_o.weight: torch.Size([768, 768])\n",
      "encoder.5.attention.W_o.bias: torch.Size([768])\n",
      "encoder.5.attention.W_q.weight: torch.Size([768, 768])\n",
      "encoder.5.attention.W_q.bias: torch.Size([768])\n",
      "encoder.5.attention.W_k.weight: torch.Size([768, 768])\n",
      "encoder.5.attention.W_k.bias: torch.Size([768])\n",
      "encoder.5.attention.W_v.weight: torch.Size([768, 768])\n",
      "encoder.5.attention.W_v.bias: torch.Size([768])\n",
      "encoder.5.LayerNorm.weight: torch.Size([768])\n",
      "encoder.5.LayerNorm.bias: torch.Size([768])\n",
      "encoder.5.intermediate.weight: torch.Size([3072, 768])\n",
      "encoder.5.intermediate.bias: torch.Size([3072])\n",
      "encoder.5.output.weight: torch.Size([768, 3072])\n",
      "encoder.5.output.bias: torch.Size([768])\n",
      "encoder.6.attention.output_LayerNorm.weight: torch.Size([768])\n",
      "encoder.6.attention.output_LayerNorm.bias: torch.Size([768])\n",
      "encoder.6.attention.W_o.weight: torch.Size([768, 768])\n",
      "encoder.6.attention.W_o.bias: torch.Size([768])\n",
      "encoder.6.attention.W_q.weight: torch.Size([768, 768])\n",
      "encoder.6.attention.W_q.bias: torch.Size([768])\n",
      "encoder.6.attention.W_k.weight: torch.Size([768, 768])\n",
      "encoder.6.attention.W_k.bias: torch.Size([768])\n",
      "encoder.6.attention.W_v.weight: torch.Size([768, 768])\n",
      "encoder.6.attention.W_v.bias: torch.Size([768])\n",
      "encoder.6.LayerNorm.weight: torch.Size([768])\n",
      "encoder.6.LayerNorm.bias: torch.Size([768])\n",
      "encoder.6.intermediate.weight: torch.Size([3072, 768])\n",
      "encoder.6.intermediate.bias: torch.Size([3072])\n",
      "encoder.6.output.weight: torch.Size([768, 3072])\n",
      "encoder.6.output.bias: torch.Size([768])\n",
      "encoder.7.attention.output_LayerNorm.weight: torch.Size([768])\n",
      "encoder.7.attention.output_LayerNorm.bias: torch.Size([768])\n",
      "encoder.7.attention.W_o.weight: torch.Size([768, 768])\n",
      "encoder.7.attention.W_o.bias: torch.Size([768])\n",
      "encoder.7.attention.W_q.weight: torch.Size([768, 768])\n",
      "encoder.7.attention.W_q.bias: torch.Size([768])\n",
      "encoder.7.attention.W_k.weight: torch.Size([768, 768])\n",
      "encoder.7.attention.W_k.bias: torch.Size([768])\n",
      "encoder.7.attention.W_v.weight: torch.Size([768, 768])\n",
      "encoder.7.attention.W_v.bias: torch.Size([768])\n",
      "encoder.7.LayerNorm.weight: torch.Size([768])\n",
      "encoder.7.LayerNorm.bias: torch.Size([768])\n",
      "encoder.7.intermediate.weight: torch.Size([3072, 768])\n",
      "encoder.7.intermediate.bias: torch.Size([3072])\n",
      "encoder.7.output.weight: torch.Size([768, 3072])\n",
      "encoder.7.output.bias: torch.Size([768])\n",
      "encoder.8.attention.output_LayerNorm.weight: torch.Size([768])\n",
      "encoder.8.attention.output_LayerNorm.bias: torch.Size([768])\n",
      "encoder.8.attention.W_o.weight: torch.Size([768, 768])\n",
      "encoder.8.attention.W_o.bias: torch.Size([768])\n",
      "encoder.8.attention.W_q.weight: torch.Size([768, 768])\n",
      "encoder.8.attention.W_q.bias: torch.Size([768])\n",
      "encoder.8.attention.W_k.weight: torch.Size([768, 768])\n",
      "encoder.8.attention.W_k.bias: torch.Size([768])\n",
      "encoder.8.attention.W_v.weight: torch.Size([768, 768])\n",
      "encoder.8.attention.W_v.bias: torch.Size([768])\n",
      "encoder.8.LayerNorm.weight: torch.Size([768])\n",
      "encoder.8.LayerNorm.bias: torch.Size([768])\n",
      "encoder.8.intermediate.weight: torch.Size([3072, 768])\n",
      "encoder.8.intermediate.bias: torch.Size([3072])\n",
      "encoder.8.output.weight: torch.Size([768, 3072])\n",
      "encoder.8.output.bias: torch.Size([768])\n",
      "encoder.9.attention.output_LayerNorm.weight: torch.Size([768])\n",
      "encoder.9.attention.output_LayerNorm.bias: torch.Size([768])\n",
      "encoder.9.attention.W_o.weight: torch.Size([768, 768])\n",
      "encoder.9.attention.W_o.bias: torch.Size([768])\n",
      "encoder.9.attention.W_q.weight: torch.Size([768, 768])\n",
      "encoder.9.attention.W_q.bias: torch.Size([768])\n",
      "encoder.9.attention.W_k.weight: torch.Size([768, 768])\n",
      "encoder.9.attention.W_k.bias: torch.Size([768])\n",
      "encoder.9.attention.W_v.weight: torch.Size([768, 768])\n",
      "encoder.9.attention.W_v.bias: torch.Size([768])\n",
      "encoder.9.LayerNorm.weight: torch.Size([768])\n",
      "encoder.9.LayerNorm.bias: torch.Size([768])\n",
      "encoder.9.intermediate.weight: torch.Size([3072, 768])\n",
      "encoder.9.intermediate.bias: torch.Size([3072])\n",
      "encoder.9.output.weight: torch.Size([768, 3072])\n",
      "encoder.9.output.bias: torch.Size([768])\n",
      "encoder.10.attention.output_LayerNorm.weight: torch.Size([768])\n",
      "encoder.10.attention.output_LayerNorm.bias: torch.Size([768])\n",
      "encoder.10.attention.W_o.weight: torch.Size([768, 768])\n",
      "encoder.10.attention.W_o.bias: torch.Size([768])\n",
      "encoder.10.attention.W_q.weight: torch.Size([768, 768])\n",
      "encoder.10.attention.W_q.bias: torch.Size([768])\n",
      "encoder.10.attention.W_k.weight: torch.Size([768, 768])\n",
      "encoder.10.attention.W_k.bias: torch.Size([768])\n",
      "encoder.10.attention.W_v.weight: torch.Size([768, 768])\n",
      "encoder.10.attention.W_v.bias: torch.Size([768])\n",
      "encoder.10.LayerNorm.weight: torch.Size([768])\n",
      "encoder.10.LayerNorm.bias: torch.Size([768])\n",
      "encoder.10.intermediate.weight: torch.Size([3072, 768])\n",
      "encoder.10.intermediate.bias: torch.Size([3072])\n",
      "encoder.10.output.weight: torch.Size([768, 3072])\n",
      "encoder.10.output.bias: torch.Size([768])\n",
      "encoder.11.attention.output_LayerNorm.weight: torch.Size([768])\n",
      "encoder.11.attention.output_LayerNorm.bias: torch.Size([768])\n",
      "encoder.11.attention.W_o.weight: torch.Size([768, 768])\n",
      "encoder.11.attention.W_o.bias: torch.Size([768])\n",
      "encoder.11.attention.W_q.weight: torch.Size([768, 768])\n",
      "encoder.11.attention.W_q.bias: torch.Size([768])\n",
      "encoder.11.attention.W_k.weight: torch.Size([768, 768])\n",
      "encoder.11.attention.W_k.bias: torch.Size([768])\n",
      "encoder.11.attention.W_v.weight: torch.Size([768, 768])\n",
      "encoder.11.attention.W_v.bias: torch.Size([768])\n",
      "encoder.11.LayerNorm.weight: torch.Size([768])\n",
      "encoder.11.LayerNorm.bias: torch.Size([768])\n",
      "encoder.11.intermediate.weight: torch.Size([3072, 768])\n",
      "encoder.11.intermediate.bias: torch.Size([3072])\n",
      "encoder.11.output.weight: torch.Size([768, 3072])\n",
      "encoder.11.output.bias: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "for key, value in state_dict.items():\n",
    "    print(f\"{key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证自己写的代码输出内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.zeros((2, 128), dtype=torch.long)  # 批次2，序列长度128\n",
    "attention_mask = torch.ones(2, 128, dtype=torch.float32)        # 无padding，全1掩码\n",
    "# attention_mask[0][-1]=0\n",
    "# attention_mask[1][-1]=0\n",
    "token_type_ids = torch.zeros(2, 128, dtype=torch.long)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bert(\n",
    "    d_model=768,\n",
    "    max_len=512,\n",
    "    vocab_size=30522,\n",
    "    d_ff=3072,\n",
    "    n_heads=12,\n",
    "    num_hidden_layers=12,\n",
    ")\n",
    "safetensor_path = \"../model/bert/model.safetensors\"\n",
    "model.load_dict(safetensor_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 768])\n",
      "tensor([-0.3210, -0.1248, -0.2172,  0.5433, -0.5840, -0.5502,  0.2830,  0.3881,\n",
      "         0.0953, -1.1299], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.1242, -0.3776, -0.8278,  ..., -0.8794, -0.6126, -0.0377],\n",
      "        [-0.1242, -0.3776, -0.8278,  ..., -0.8794, -0.6126, -0.0377]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "embedding_output = model.embeddings(input_ids, token_type_ids)\n",
    "encoder_output = model(input_ids, token_type_ids, attention_mask)\n",
    "# print(embedding_output[0][0][:2])\n",
    "print(encoder_output[0].shape)  # MLM logits\n",
    "print(encoder_output[0][0][0][:10])\n",
    "print(encoder_output[1])  # NSP logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证官方输出内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "model_path = \"../model/bert\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertModel.from_pretrained(model_path)\n",
    "encoder_output = model(input_ids, attention_mask, token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 768])\n",
      "tensor([[[-0.3210, -0.1248, -0.2172,  ..., -0.5346,  0.7774, -0.2815],\n",
      "         [-0.2491, -0.1476, -0.1710,  ..., -0.5708,  0.7885, -0.1987],\n",
      "         [-0.2603, -0.1823, -0.1663,  ..., -0.5687,  0.7649, -0.2008],\n",
      "         ...,\n",
      "         [-0.3194, -0.2450, -0.0910,  ..., -0.5497,  0.8502, -0.1324],\n",
      "         [-0.3259, -0.2308, -0.0936,  ..., -0.5407,  0.8343, -0.1311],\n",
      "         [-0.3107, -0.1982, -0.0991,  ..., -0.5395,  0.8027, -0.1485]],\n",
      "\n",
      "        [[-0.3210, -0.1248, -0.2172,  ..., -0.5346,  0.7774, -0.2815],\n",
      "         [-0.2491, -0.1476, -0.1710,  ..., -0.5708,  0.7885, -0.1987],\n",
      "         [-0.2603, -0.1823, -0.1663,  ..., -0.5687,  0.7649, -0.2008],\n",
      "         ...,\n",
      "         [-0.3194, -0.2450, -0.0910,  ..., -0.5497,  0.8502, -0.1324],\n",
      "         [-0.3259, -0.2308, -0.0936,  ..., -0.5407,  0.8343, -0.1311],\n",
      "         [-0.3107, -0.1982, -0.0991,  ..., -0.5395,  0.8027, -0.1485]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "tensor([[-0.1242, -0.3776, -0.8278,  ..., -0.8794, -0.6126, -0.0377],\n",
      "        [-0.1242, -0.3776, -0.8278,  ..., -0.8794, -0.6126, -0.0377]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.zeros((2, 128), dtype=torch.long)  # 批次2，序列长度128\n",
    "attention_mask = torch.ones(2, 128, dtype=torch.float32)        # 无padding，全1掩码\n",
    "token_type_ids = torch.zeros(2, 128, dtype=torch.long)          # 单段落，全0\n",
    "# attention_mask[0][-1]=0\n",
    "# attention_mask[1][-1]=0\n",
    "sequence_output, pooled_output = model(input_ids, attention_mask, token_type_ids)\n",
    "encoder_output = model(input_ids, attention_mask, token_type_ids)\n",
    "print(encoder_output[0].shape)  # MLM logits\n",
    "print(encoder_output[0])\n",
    "print(encoder_output[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Embedding.forward() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[202], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m encoder_output \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids,token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(embedding_output[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[1;32md:\\自提高\\LLM\\transformer_self\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\自提高\\LLM\\transformer_self\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: Embedding.forward() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "embedding_output = model.embeddings(input_ids=input_ids,token_type_ids=token_type_ids)\n",
    "encoder_output = model(input_ids=input_ids,token_type_ids=token_type_ids,attention_mask=attention_mask)\n",
    "print(embedding_output[0][0][:2])\n",
    "print(encoder_output[0].shape)\n",
    "print(encoder_output[0][0][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 官方模型框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file, load_file\n",
    "loaded_state = load_file(\"../model/bert/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.LayerNorm.beta torch.Size([768])\n",
      "bert.embeddings.LayerNorm.gamma torch.Size([768])\n",
      "bert.embeddings.position_embeddings.weight torch.Size([512, 768])\n",
      "bert.embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
      "bert.embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.0.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.0.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.1.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.1.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.10.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.10.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.11.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.11.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.2.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.2.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.3.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.3.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.4.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.4.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.5.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.5.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.6.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.6.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.7.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.7.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.8.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.8.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "bert.encoder.layer.9.output.LayerNorm.beta torch.Size([768])\n",
      "bert.encoder.layer.9.output.LayerNorm.gamma torch.Size([768])\n",
      "bert.encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "bert.pooler.dense.bias torch.Size([768])\n",
      "bert.pooler.dense.weight torch.Size([768, 768])\n",
      "cls.predictions.bias torch.Size([30522])\n",
      "cls.predictions.transform.LayerNorm.beta torch.Size([768])\n",
      "cls.predictions.transform.LayerNorm.gamma torch.Size([768])\n",
      "cls.predictions.transform.dense.bias torch.Size([768])\n",
      "cls.predictions.transform.dense.weight torch.Size([768, 768])\n",
      "cls.seq_relationship.bias torch.Size([2])\n",
      "cls.seq_relationship.weight torch.Size([2, 768])\n"
     ]
    }
   ],
   "source": [
    "for key, value in loaded_state.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
